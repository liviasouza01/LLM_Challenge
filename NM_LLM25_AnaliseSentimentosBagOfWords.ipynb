{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liviasouza01/LLM_Challenge/blob/main/NM_LLM25_AnaliseSentimentosBagOfWords.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9A4HuVIYGgXg"
      },
      "source": [
        "# Processo Seletivo Engenheiro de LLM\n",
        "\n",
        "\n",
        "versão 19 de janeiro de 2025"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktJJqQ4SjvB0"
      },
      "source": [
        "### Nome: Livia Souza\n",
        "\n",
        "### E-mail: lsa5@cin.ufpe.br"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRnOFwqk23W4"
      },
      "source": [
        "## Instalação e importação de pacotes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HA5BWLDCKmw3"
      },
      "outputs": [],
      "source": [
        "!pip install datasets -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VorDvF62iyXF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "from collections import Counter\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from datasets import load_dataset\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5ovJE02CwKT"
      },
      "source": [
        "## I - Vocabulário e Tokenização"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8IQxzbyZyA1"
      },
      "source": [
        "### Exemplo do dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtwgK6vnZnQ8",
        "outputId": "a5d3032a-c710-4498-d184-d83c8feda762"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "train_dataset = load_dataset(\"stanfordnlp/imdb\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fT1fRvkEIavn",
        "outputId": "5b852e51-8694-4ead-b113-608932888aee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "len(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hdWcyur0YVwl"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'<br\\s*/?>', ' ', text)\n",
        "    text = re.sub(r'[^a-z\\s\\']', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    text = re.sub(r\"\\'s\", \" is\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'m\", \" am\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'t\", \" not\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retirei a função *encode_sentence* que serviria de base para um tokenizador porque o modelo em si é simples, assim como o dataset, que apesar de grande, contém principalmente texto informal com vocabulário limitado. Portanto, o uso de um tokenizador (como BERT por exemplo) mais robusto seria desnecessário.\n",
        "\n",
        "Adicionei um pré-processamento de texto simples, que é suficiente para bag of words e tem um processamento mais rápido, servindo como um tokenizador implícito. Vale salientar que bag of words a ordem das palavras é ignorada e o foco é na frequencia de cada palavra.\n",
        "\n",
        "Além disso reduzi o tamanho do vocabulário para chegar no tempo de processamento desejado."
      ],
      "metadata": {
        "id": "5N5C0JJVRGIX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqzUqy3diz0X",
        "outputId": "30ccd649-25d0-4cb7-99b0-50d0bd19afef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building vocabulary: 100%|██████████| 25000/25000 [00:05<00:00, 4347.57it/s]\n"
          ]
        }
      ],
      "source": [
        "# limit the vocabulary size to 20000 most frequent tokens\n",
        "vocab_size = 5000\n",
        "\n",
        "counter = Counter()\n",
        "for sample in tqdm(train_dataset, desc=\"Building vocabulary\"):\n",
        "    counter.update(preprocess_text(sample[\"text\"]).split())\n",
        "\n",
        "# create a vocabulary of the 20000 most frequent tokens\n",
        "most_frequent_words = sorted(counter, key=counter.get, reverse=True)[:vocab_size]\n",
        "vocab = {word: i for i, word in enumerate(most_frequent_words, 1)}\n",
        "vocab_size = len(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iV4bF8cDAj1"
      },
      "source": [
        "## II - Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modifiquei a classe IMDBDataset para fazer um processamento em batch, o que reduz o overhead de processamento individual."
      ],
      "metadata": {
        "id": "0OMncrh9S1W7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VDUyZoTPi262"
      },
      "outputs": [],
      "source": [
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, data, vocab):\n",
        "        self.data = data\n",
        "        self.vocab = vocab\n",
        "\n",
        "        #Pre-process all data at once\n",
        "        texts = [preprocess_text(sample[\"text\"]) for sample in data]\n",
        "        labels = [1 if sample[\"label\"] == 1 else 0 for sample in data]\n",
        "\n",
        "        #Vectorize all texts at once\n",
        "        self.processed_data = []\n",
        "        for text, label in zip(texts, labels):\n",
        "            encoded = torch.zeros(len(self.vocab) + 1)\n",
        "            for word in text.split():\n",
        "                word_idx = self.vocab.get(word, 0)\n",
        "                encoded[word_idx] = 1\n",
        "            self.processed_data.append((encoded, torch.tensor(label)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.processed_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.processed_data[idx]\n",
        "\n",
        "# Divisão treino/validação\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_subset, val_subset = torch.utils.data.random_split(\n",
        "    train_dataset, [train_size, val_size]\n",
        ")\n",
        "\n",
        "# Criação dos datasets\n",
        "train_data = IMDBDataset(train_subset, vocab)\n",
        "val_data = IMDBDataset(val_subset, vocab)\n",
        "test_data = IMDBDataset(load_dataset(\"stanfordnlp/imdb\", split=\"test\"), vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7RMPSvMDL5U"
      },
      "source": [
        "## III - Data Loader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui otimizei a paralelização para 2 *workers*, que tráz um bom equilíbrio entre o paralelismo e o overhead. Além disso, adicionei o parâmetro *pin_memory* para otimizar a transferência de dados da CPU para a GPU, reduzindo o overhead de comunicação.\n"
      ],
      "metadata": {
        "id": "4JcohQH3TSNY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Y7tcZv2YDIog"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True,\n",
        "                         num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False,\n",
        "                       num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False,\n",
        "                        num_workers=2, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwPeJ7h8DahT"
      },
      "source": [
        "## IV - Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6QuDhWvji7lt"
      },
      "outputs": [],
      "source": [
        "class OneHotMLP(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(OneHotMLP, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(vocab_size+1, 200)\n",
        "        self.fc2 = nn.Linear(200, 1)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        o = self.fc1(x.float())\n",
        "        o = self.relu(o)\n",
        "        return self.fc2(o)\n",
        "\n",
        "# Model instantiation\n",
        "model = OneHotMLP(vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVAhdFGXDepU"
      },
      "source": [
        "## V - Laço de Treinamento - Otimização da função de Perda pelo Gradiente descendente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaH1Uv3yHih5",
        "outputId": "076004ed-70a3-49cf-cf00-8fd24de96516"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# Verifica se há uma GPU disponível e define o dispositivo para GPU se possível,\n",
        "# caso contrário, usa a CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if device.type == 'cuda':\n",
        "    print('GPU:', torch.cuda.get_device_name(torch.cuda.current_device()))\n",
        "else:\n",
        "    print('using CPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Durante a validação, não precisamos fazer backpropagation porque não estamos atualizando os pesos do modelo, então usar *torch.no_grad()* evita cálculos desnecessários e assim é possível reduzir tempo de processamento."
      ],
      "metadata": {
        "id": "Fig_fNW3UKVm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nh_pe8rni93_",
        "outputId": "63526c65-2cf1-4c16-a773-620dec487ba8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Train Loss: 0.6395, Val Loss: 0.5522, Elapsed Time: 1.66s\n",
            "Epoch [2/5], Train Loss: 0.4606, Val Loss: 0.4232, Elapsed Time: 1.32s\n",
            "Epoch [3/5], Train Loss: 0.3625, Val Loss: 0.3565, Elapsed Time: 1.33s\n",
            "Epoch [4/5], Train Loss: 0.3218, Val Loss: 0.3679, Elapsed Time: 1.33s\n",
            "Epoch [5/5], Train Loss: 0.2932, Val Loss: 0.3237, Elapsed Time: 1.41s\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "model = model.to(device)\n",
        "# Define loss and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.05)\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    num_train_batches = 0\n",
        "\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.squeeze(), targets.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "        num_train_batches += 1\n",
        "\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    num_val_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs.squeeze(), targets.float())\n",
        "            total_val_loss += loss.item()\n",
        "            num_val_batches += 1\n",
        "\n",
        "    # Calculate average losses\n",
        "    avg_train_loss = total_train_loss / num_train_batches\n",
        "    avg_val_loss = total_val_loss / num_val_batches\n",
        "    epoch_time = time.time() - start_time\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
        "          f'Train Loss: {avg_train_loss:.4f}, '\n",
        "          f'Val Loss: {avg_val_loss:.4f}, '\n",
        "          f'Elapsed Time: {epoch_time:.2f}s')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bwvahen5D1oM"
      },
      "source": [
        "## VI - Avaliação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DtTPUBfjBj-",
        "outputId": "442e25ad-2d6a-4332-9b9a-54c5bb35dacf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 86.936%\n"
          ]
        }
      ],
      "source": [
        "## evaluation\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        logits = model(inputs)\n",
        "        predicted = torch.round(torch.sigmoid(logits.squeeze()))\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "\n",
        "    print(f'Test Accuracy: {100 * correct / total}%')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}