{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liviasouza01/LLM_Challenge/blob/main/C%C3%B3pia_de_NM_LLM25_AnaliseSentimentosBagOfWords.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9A4HuVIYGgXg"
      },
      "source": [
        "# Processo Seletivo Engenheiro de LLM\n",
        "\n",
        "\n",
        "versão 19 de janeiro de 2025"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktJJqQ4SjvB0"
      },
      "source": [
        "### Nome: Livia Souza\n",
        "\n",
        "### E-mail: lsa5@cin.ufpe.br"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRnOFwqk23W4"
      },
      "source": [
        "## Instalação e importação de pacotes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "HA5BWLDCKmw3"
      },
      "outputs": [],
      "source": [
        "!pip install datasets -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "VorDvF62iyXF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "from collections import Counter\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from datasets import load_dataset\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "import time\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5ovJE02CwKT"
      },
      "source": [
        "## I - Vocabulário e Tokenização"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8IQxzbyZyA1"
      },
      "source": [
        "### Exemplo do dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "MtwgK6vnZnQ8"
      },
      "outputs": [],
      "source": [
        "train_dataset = load_dataset(\"stanfordnlp/imdb\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fT1fRvkEIavn",
        "outputId": "76e5ecfb-fef2-4e1e-e2e1-85165f11d714"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "len(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "hdWcyur0YVwl"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retirei a função *encode_sentence* e usei o AutoTokenizer da Huggingface\n",
        "\n",
        "Escolhi o modelo BERT-base-uncased para realizar a tokenização do texto. Esse modelo divide palavras desconhecidas em subpalavras mais comuns, o que reduz problemas com palavras raras e melhora a generalização do modelo.\n",
        "\n",
        "Além disso reduzi o tamanho do vocabulário para reduzir a dimensionalidade de entrada e assim o tempo de processamento."
      ],
      "metadata": {
        "id": "5N5C0JJVRGIX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqzUqy3diz0X",
        "outputId": "69b4e4e1-343d-4e35-ca5a-b661707d8a14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building vocabulary:   0%|          | 0/25000 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (718 > 512). Running this sequence through the model will result in indexing errors\n",
            "Building vocabulary: 100%|██████████| 25000/25000 [01:11<00:00, 351.90it/s]\n"
          ]
        }
      ],
      "source": [
        "# limit the vocabulary size to 20000 most frequent tokens\n",
        "vocab_size = 5000\n",
        "\n",
        "counter = Counter()\n",
        "for sample in tqdm(train_dataset, desc=\"Building vocabulary\"):\n",
        "    tokens = tokenizer.tokenize(sample[\"text\"])\n",
        "    counter.update(tokens)\n",
        "\n",
        "# Criando vocabulário\n",
        "most_frequent_words = [word for word, _ in counter.most_common(vocab_size)]\n",
        "vocab = {word: i for i, word in enumerate(most_frequent_words, 1)}\n",
        "vocab_size = len(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iV4bF8cDAj1"
      },
      "source": [
        "## II - Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A implementação garante que o processo de tokenização, vetorização e a construção de conjuntos de dados (train, val, test) seja alinhado com os requisitos do projeto. Dessa forma, a estrutura de dados fica pronta para ser usada no treinamento."
      ],
      "metadata": {
        "id": "0OMncrh9S1W7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "VDUyZoTPi262"
      },
      "outputs": [],
      "source": [
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, data, vocab):\n",
        "        self.vocab = vocab\n",
        "        self.processed_data = []\n",
        "\n",
        "        for sample in data:\n",
        "            text = sample[\"text\"]\n",
        "            label = 1 if sample[\"label\"] == 1 else 0\n",
        "\n",
        "            # Tokenizar o texto\n",
        "            tokens = tokenizer.tokenize(text)\n",
        "\n",
        "            # Vetorização one-hot\n",
        "            encoded = torch.zeros(len(vocab) + 1)\n",
        "            for token in tokens:\n",
        "                word_idx = vocab.get(token, 0)  # 0 para OOV\n",
        "                encoded[word_idx] = 1\n",
        "\n",
        "            self.processed_data.append((encoded, torch.tensor(label)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.processed_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.processed_data[idx]\n",
        "\n",
        "# Divisão treino/validação\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_subset, val_subset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "# Criar datasets\n",
        "train_data = IMDBDataset(train_subset, vocab)\n",
        "val_data = IMDBDataset(val_subset, vocab)\n",
        "test_data = IMDBDataset(load_dataset(\"stanfordnlp/imdb\", split=\"test\"), vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7RMPSvMDL5U"
      },
      "source": [
        "## III - Data Loader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui otimizei a paralelização para 2 *workers*, que tráz um bom equilíbrio entre o paralelismo e o overhead. Além disso, adicionei o parâmetro *pin_memory* para otimizar a transferência de dados da CPU para a GPU, reduzindo o overhead de comunicação.\n"
      ],
      "metadata": {
        "id": "4JcohQH3TSNY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Y7tcZv2YDIog"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwPeJ7h8DahT"
      },
      "source": [
        "## IV - Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "6QuDhWvji7lt"
      },
      "outputs": [],
      "source": [
        "class OneHotMLP(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(OneHotMLP, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(vocab_size+1, 200)\n",
        "        self.fc2 = nn.Linear(200, 1)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        o = self.fc1(x.float())\n",
        "        o = self.relu(o)\n",
        "        return self.fc2(o)\n",
        "\n",
        "# Model instantiation\n",
        "model = OneHotMLP(vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVAhdFGXDepU"
      },
      "source": [
        "## V - Laço de Treinamento - Otimização da função de Perda pelo Gradiente descendente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaH1Uv3yHih5",
        "outputId": "3cf6a10d-100f-4b80-cf41-3c50e00e1b23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using CPU\n"
          ]
        }
      ],
      "source": [
        "# Verifica se há uma GPU disponível e define o dispositivo para GPU se possível,\n",
        "# caso contrário, usa a CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if device.type == 'cuda':\n",
        "    print('GPU:', torch.cuda.get_device_name(torch.cuda.current_device()))\n",
        "else:\n",
        "    print('using CPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Fig_fNW3UKVm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nh_pe8rni93_",
        "outputId": "9a8f6c0f-0fe4-4477-e7e5-5b2e4f66f812"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Train Loss: 0.6892, Val Loss: 0.6833, Time: 5.71s\n",
            "Epoch [2/5], Train Loss: 0.6749, Val Loss: 0.6645, Time: 4.04s\n",
            "Epoch [3/5], Train Loss: 0.6498, Val Loss: 0.6326, Time: 3.96s\n",
            "Epoch [4/5], Train Loss: 0.6110, Val Loss: 0.5875, Time: 4.73s\n",
            "Epoch [5/5], Train Loss: 0.5613, Val Loss: 0.5362, Time: 4.07s\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "model = model.to(device)\n",
        "# Define loss and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()  # Inicia contagem do tempo\n",
        "\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    num_train_batches = 0\n",
        "\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.squeeze(), targets.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "        num_train_batches += 1\n",
        "\n",
        "    # Validação\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    num_val_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs.squeeze(), targets.float())\n",
        "            total_val_loss += loss.item()\n",
        "            num_val_batches += 1\n",
        "\n",
        "    avg_train_loss = total_train_loss / num_train_batches\n",
        "    avg_val_loss = total_val_loss / num_val_batches\n",
        "    epoch_time = time.time() - start_time  # Calcula tempo da época\n",
        "\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
        "          f'Train Loss: {avg_train_loss:.4f}, '\n",
        "          f'Val Loss: {avg_val_loss:.4f}, '\n",
        "          f'Time: {epoch_time:.2f}s')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bwvahen5D1oM"
      },
      "source": [
        "## VI - Avaliação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DtTPUBfjBj-",
        "outputId": "4a45bee6-9fee-41cf-c631-1b0f81357706"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 80.448%\n"
          ]
        }
      ],
      "source": [
        "## evaluation\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        logits = model(inputs)\n",
        "        predicted = torch.round(torch.sigmoid(logits.squeeze()))\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "\n",
        "    print(f'Test Accuracy: {100 * correct / total}%')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}